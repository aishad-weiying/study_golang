# 斗鱼图片爬取分析

1. url
打开斗鱼的官网,选择颜值分类,可以得到网址
https://www.douyu.com/g_yz

2. 图片对应的标签
在任意的一张图片上点击审查元素,得到该图片的数据信息
```html
<img src="https://rpic.douyucdn.cn/live-cover/appCovers/2019/11/02/6259669_20191102185013_small.jpg/dy1" class="DyImg-content is-normal ">
```

查看网址源代码可以看到,图片的格式为
```html
<img loading="lazy" src="图片的 url" class="DyImg-content is-normal"
```

### 具体的实现过程

1. 让用户输入要爬取的图片个数的范围

2. 根据范围循环爬取指定个数的图片

3. 爬取整个页面的信息,保存在指定的变量中

4. 使用正则表达式提取图片的网址

5. 根据提取到的图片的网址,获取数据,保存在本地

```go
package main

import (
	"fmt"
	"io"
	"net/http"
	"os"
	"regexp"
	"strconv"
)

func main() {
	var num int
	fmt.Printf("请输入要下载多少张图片：")
	fmt.Scan(&num)

	// 爬取页面的url
	url := "https://www.douyu.com/g_yz"

	//调用函数，获取图
	result, err := dy_httpGet(url)
	if err != nil {
		fmt.Println(err)
		return
	}
	//fmt.Println(result)

	// 解析提取网址的正则表达式
	res := regexp.MustCompile(`<img loading="lazy" src="(?s:(.*?))" class="DyImg-content`)

	// 提取url
	alls := res.FindAllStringSubmatch(result, num)

	// 提取图片并保存在文件中
	// 创建管道，使go程在完全爬取完数据有才结束
	page := make(chan int)
	for n , images := range alls {
		// 读取文件并保存
		go spiderImage(images[1],n,page)
	}

	for i:=1 ; i <=len(alls) ; i++ {
		fmt.Printf("第%d张处理完毕！！\n", <-page + 1)
	}
}

// 获取整个页面的数据
func dy_httpGet(url string) (result string, err error) {
	res, err1 := http.Get(url)
	if err1 != nil {
		err = err1
		return
	}
	defer res.Body.Close()
	buf := make([]byte, 4096)
	for {
		n, err2 := res.Body.Read(buf)
		if n == 0 {
			break
		}
		if err2 != nil && err2 != io.EOF {
			err = err2
			return
		}
		result += string(buf[:n])
	}
	return
}

func spiderImage(url string, index int,page chan int)  {
	//// 创建文件
	f ,err := os.Create("/Users/weiying/Desktop/crawler/" + "第" + strconv.Itoa(index+1) + "张" + ".png")
	defer f.Close()
	if err != nil {
		fmt.Println("os.creat error",err)
		return
	}
	// 发起请求
	res , err := http.Get(url)
	if err != nil {
		fmt.Println("http.Get image",err)
		return
	}
	defer res.Body.Close()
	buf := make([]byte,4096)
	// 保存
	for {
		n , err := res.Body.Read(buf)
		if n == 0{
			break
		}
		if err != nil {
			fmt.Println("res.body.read",err)
			return
		}
		f.Write(buf[:n])
	}
	page <- index
}
```