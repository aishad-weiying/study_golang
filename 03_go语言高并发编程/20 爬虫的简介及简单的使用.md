# 爬虫简介
上面文章中编写的模拟客户端向服务器请求数据的程序,其实就是一个爬虫,可以将网络数据爬取到本地,只不过我只是将所有的数据全部爬取下来,没有进行任何的筛选和处理

#### 爬虫的定义
网络爬虫(又被称为网页蜘蛛或者网络机器人),是一种按照一定的规则自动的抓取万维网信息的程序

简单的来说,就是编写程序,模拟浏览器的访问行为发送请求,获取到和浏览器一样的数据,因此,我们能获取到的是浏览器能够接收到的数据

#### 爬虫的用途

- 呈现数据,呈现在 app 或者网站上

- 进行数据分析,或得结果

#### 爬虫的分类

- 通用爬虫:搜索引擎的爬虫

- 聚焦爬虫:针对特定网站的爬虫

#### 聚焦爬虫的工作流程

1. 明确 URL(请求的地址,明确爬取什么数据)

2. 发送请求,获得响应数据

3. 保存响应数据,提取有用的信息

4. 处理数据(存储和使用)


## 爬取百度贴吧的例子

1. 请求的 URL,这里我们以吃鸡游戏吧为例,分析地址规则
```bash
https://tieba.baidu.com/f?kw=%E7%BB%9D%E5%9C%B0%E6%B1%82%E7%94%9F&ie=utf-8&pn=0  // 第一页
https://tieba.baidu.com/f?kw=%E7%BB%9D%E5%9C%B0%E6%B1%82%E7%94%9F&ie=utf-8&pn=50 //第二页
https://tieba.baidu.com/f?kw=%E7%BB%9D%E5%9C%B0%E6%B1%82%E7%94%9F&ie=utf-8&pn=100  //第三页
```
> 规律也就是下一页的地址也就是前一页地址+50

2. 发送请求,获取响应 (将所有的网站的内容全部爬下来)

3. 提取数据

4. 处理数据

#### 简单的贴吧页面爬取代码
```go
package main

import (
	"fmt"
	"io"
	"net/http"
	"os"
	"strconv"
)

func HttpGet(start , end int)  {
	fmt.Printf("正在爬取第%d页到%d页的数据\n",start,end)

	for i := start ; i <= end ; i++ {
		// 爬取页面的url
		url := "https://tieba.baidu.com/f?kw=%E7%BB%9D%E5%9C%B0%E6%B1%82%E7%94%9F&ie=utf-8&pn=" + strconv.Itoa((i-1)*50)
		fmt.Printf("正在爬取第%d页的数据\n",i)
		//封装函数完成爬取工作
		result , err := Crawler(url)
		if err != nil {
			fmt.Println("Crawler error :",err)
			continue //如果爬取本页面出错，继续爬取下一个页面
		}
		//将爬取的数据保存在文件中
		f , err := os.Create("第"+ strconv.Itoa(i)+"页"+".html")
		if err != nil {
			fmt.Println("os.Create error :",err)
			return
		}
		defer f.Close()
		f.WriteString(result)
	}
}

func Crawler(url string) (result string , err error)  {
	// 发送请求
	response , err1 := http.Get(url)
	if err1 != nil{
		err = err1
		return
	}
	defer response.Body.Close()  // 结束的时候关闭
	buf := make([]byte,4096)

	for {
		n , err2 := response.Body.Read(buf)
		if n == 0 {
			fmt.Println("爬取本页完毕")
			break
		}
		if err2 != nil && err2 != io.EOF {
			err = err2
			return
		}
		result += string(buf[:n])
	}
	return
}

func main()  {
	var start , end int
	fmt.Printf("请输入起始页（>=1）：")
	fmt.Scan(&start)
	fmt.Printf("请输入结束页（>=start）：")
	fmt.Scan(&end)
	//封装函数，完成爬虫任务
	HttpGet(start,end)
}
```

#### 并发百度贴吧代码
上面的代码,只是单进程爬取的,当我们选择爬取 10 页以后,只能是一页页的爬取,使用 go 程完成并发爬取,因为在 go  语言中 goroutine 十分轻量级,所以如果我们要爬取 n 也的数据,那么我们直接使用 n 个 go 程去爬取数据完成并发
```go
package main

import (
	"fmt"
	"io"
	"net/http"
	"os"
	"strconv"
)

// 创建爬取页面的go程
func Crawler(i int,page chan int ) {
	// 爬取页面的url
	url := "https://tieba.baidu.com/f?kw=%E7%BB%9D%E5%9C%B0%E6%B1%82%E7%94%9F&ie=utf-8&pn=" + strconv.Itoa((i-1)*50)
	fmt.Printf("正在爬取第%d页的数据\n",i)
	//封装函数完成爬取工作
	result , err := HttpGet(url)
	if err != nil {
		fmt.Println("Crawler error :",err)
		return
	}
	//将爬取的数据保存在文件中
	f , err := os.Create("/Users/weiying/Desktop/crawler/"+"第"+ strconv.Itoa(i)+"页"+".html")
	if err != nil {
		fmt.Println("os.Create error :",err)
		return
	}
	defer f.Close()
	f.WriteString(result)
	page <- i
}
// 根据指定的爬取页面,创建对应的 go 程
func worker(start , end int)  {
	fmt.Printf("正在爬取第%d页到%d页的数据\n",start,end)

	// 创建管道，使主go程在完全爬取完数据有才结束
	page := make(chan int)
	for i := start ; i <= end ; i++ {
		go Crawler(i,page)
	}
	for i := start ; i <= end ; i++ {
		fmt.Printf("爬取%d页完毕\n",<-page)
	}
}

// 爬取数据,并将数据返回给 go 程
func HttpGet(url string) (result string , err error)  {
	// 发送请求
	response , err1 := http.Get(url)
	if err1 != nil{
		err = err1
		return
	}
	defer response.Body.Close()  // 结束的时候关闭
	buf := make([]byte,4096)

	for {
		n , err2 := response.Body.Read(buf)
		if n == 0 {
			break
		}
		if err2 != nil && err2 != io.EOF {
			err = err2
			return
		}
		result += string(buf[:n])
	}
	return
}

func main()  {
	var start , end int
	fmt.Printf("请输入起始页（>=1）：")
	fmt.Scan(&start)
	fmt.Printf("请输入结束页（>=start）：")
	fmt.Scan(&end)
	//封装函数，完成爬虫任务
	worker(start,end)
}
```