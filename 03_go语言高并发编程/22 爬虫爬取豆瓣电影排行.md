# 双向爬取

1. 横向爬取
所谓的横向爬取,是指在爬取的网站页面中,以"页"为单位,搜寻该网站分页器的规律,一页一页的爬取网站数据,大多数的网站,采用分页管理模式,针对这类网站,首先要确定横向爬取方法

2. 纵向爬取
纵向爬取,是指在一个页面内,按照不同的"条目"为单位,找寻各个条目间的规律,一条一条的爬取一个页面中的数据信息,也就是同时爬取一个页面内不同类别的数据

## 爬取豆瓣电影评分
在豆瓣电影的排行榜中,有一个 top 250 的排行榜,URL 为https://movie.douban.com/top250

#### 横向爬取的规律
第一页: https://movie.douban.com/top250?start=0&filter=
第二页:https://movie.douban.com/top250?start=25&filter=
第三页:https://movie.douban.com/top250?start=50&filter=

根据上面,能够得出规律,下一页是在前一页的基础上 start 的值+25

#### 纵向爬取的规律
对于电影的评分来说,重要的就是电影名, 评分和评价人数,查看页面的代码分析
```html
<<div class="item">
<div class="pic">
	<em class="">51</em>
	<a href="https://movie.douban.com/subject/1299131/">
		<img width="100" alt="教父2" src="https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2194138787.webp" class=""> //电影的图片
	</a>
</div>
<div class="info">
	<div class="hd">
		<a href="https://movie.douban.com/subject/1299131/" class="">
			<span class="title">教父2</span> // 电影的名称
			<span class="title"> / The Godfather: Part Ⅱ</span> //电影的英文名
			<span class="other"> / 教父续集  /  教父II</span>
		</a>
		<span class="playable">[可播放]</span>
	</div>
	<div class="bd">
		<p class="">
			导演: 弗朗西斯·福特·科波拉 Francis Ford Coppola   主演: 阿尔·帕西诺 A...<br>
			1974&n bsp;/ 美国 / 剧情 犯罪
		</p>
		<div class="star">
			<span class="rating45-t"></span>
			<span class="rating_num" property="v:average">9.2</span> //电影的评分
			<span property="v:best" content="10.0"></span> 
			<span>362246人评价</span> //电影的评价人数
		</div>
		<p class="quote">
			<span class="inq">优雅的孤独。</span>
		</p>
	</div>
</div>
</div>
</li>
<li>
```
获取主要要素的标记
```go
电影名称 :<img width="100" alt="教父2"  ----> 正则表达式写法`<img width="100" alt="(.*?)"`
电影评分:<span class="rating_num" property="v:average">9.2</span> ----> 正则表达式写法`<span class="rating_num" property="v:average">(?s:(.*?))</span>`
评价人数:<span>362246人评价</span> ----> 正则表达式写法`<span>(.*?)人评价</span>`
```
### 代码实现
1. 创建主进程,让用户选择要的指定页面 1-10

2. 循环请求页面的数据

3. 获取数据后,经过正则表达式匹配,写入到文件中

```go
package main

import (
	"fmt"
	"io"
	"net/http"
	"os"
	"regexp"
	"strconv"
)

func DB_worker(start , end int)  {
	fmt.Printf("正在爬取第%d页到%d页的数据\n",start,end)
	//循环爬取页面数据
	for i := start ; i<= end ; i++ {
		// 爬取页面的url
		url := "https://movie.douban.com/top250?start=" + strconv.Itoa((i-1)*25) + "&filter="
		fmt.Printf("正在爬取第%d页的数据\n",i)
		//封装函数完成爬取工作
		result , err := DB_crawler(url)
		if err != nil {
			fmt.Println("DB_crawlwe error:",err)
			continue
		}
		// 解析， 编译正则表达式 -- 电影名称
		res1 := regexp.MustCompile(`<img width="100" alt="(.*?)"`)
		// 提取电影名信息
		filmname :=res1.FindAllStringSubmatch(result,-1)
		// 解析， 编译正则表达式 -- 电影评分
		res2 := regexp.MustCompile(`<span class="rating_num" property="v:average">(?s:(.*?))</span>`)
		// 提取电影评分信息
		filmscore :=res2.FindAllStringSubmatch(result,-1)
		// 解析， 编译正则表达式 -- 评分人数
		res3 := regexp.MustCompile(`<span>(.*?)人评价</span>`)
		// 提取评分人数信息
		people :=res3.FindAllStringSubmatch(result,-1)

		//将筛选出的数据保存到文件中
		SaveTofile(filmname,filmscore,people, i)
	}

}

// 爬取单个页面的函数
func DB_crawler(url string)(result string,err error)  {
	// 向指定的url发送请求
	response , err1 := http.Get(url)
	if err1 != nil {
		err = err1
		return
	}
	defer response.Body.Close()
	buf := make([]byte,4096)
	for {
		n , err2 := response.Body.Read(buf)
		if n == 0 {
			fmt.Println("爬取本页完毕")
			break
		}
		if err2 !=nil && err2 != io.EOF {
			err = err2
			return
		}
		result = result + string(buf[:n])
	}
	return
}
//分析写入结果的函数
func SaveTofile(filmname,filmscore,people [][]string , index int)  {
	//将爬取的数据保存在文件中
	f , err := os.Create("/Users/weiying/Desktop/crawler/"+"第"+ strconv.Itoa(index)+"页"+".txt")
	if err != nil {
		fmt.Println("os.Create error :",err)
		return
	}
	defer f.Close()
	for i :=1 ; i <= len(filmname) ; i++ {
		f.WriteString("第" + strconv.Itoa(i) + "名：" + filmname[i][1]+"\t\t\t" +
			filmscore[i][1] +  "\t\t\t" + people[i][1])
	}
	fmt.Printf("第%d页分析写入完毕\n",index)
}

func main() {
	var start , end int
	fmt.Printf("请输入起始页（>=1）：")
	fmt.Scan(&start)
	fmt.Printf("请输入结束页（>=start）：")
	fmt.Scan(&end)
	//封装函数，完成爬虫任务
	DB_worker(start,end)
}
```

## 并发爬取数据
```go
package main

import (
	"fmt"
	"io"
	"net/http"
	"os"
	"regexp"
	"strconv"
)

func DB_worker(start, end int) {
	fmt.Printf("正在爬取第%d页到%d页的数据\n", start, end)
	// 创建管道，使主go程在完全爬取完数据有才结束
	page := make(chan int)
	//循环爬取页面数据
	for i := start; i <= end; i++ {
		// 爬取页面的url
		url := "https://movie.douban.com/top250?start=" + strconv.Itoa((i-1)*25) + "&filter="
		fmt.Printf("正在爬取第%d页的数据\n", i)
		//封装函数完成爬取工作
		go DB_crawler(url, i, page)
	}
	for i := start; i <= end; i++ {
		fmt.Printf("第%d页处理完毕！！\n", <-page)
	}

}

// 爬取单个页面的函数
func DB_crawler(url string, index int, page chan int) {
	// 向指定的url发送请求
	response, err := http.Get(url)
	if err != nil {
		fmt.Println("http.Get error:",err)
		return
	}
	defer response.Body.Close()
	var result string
	buf := make([]byte, 4096)
	for {
		n, err := response.Body.Read(buf)
		if n == 0 {
			break
		}
		if err != nil && err != io.EOF {
			fmt.Println("http.Get error:",err)
			return
		}
		result = result + string(buf[:n])
	}
	// 解析， 编译正则表达式 -- 电影名称
	res1 := regexp.MustCompile(`<img width="100" alt="(.*?)"`)
	// 提取电影名信息
	filmname := res1.FindAllStringSubmatch(result, -1)
	// 解析， 编译正则表达式 -- 电影评分
	res2 := regexp.MustCompile(`<span class="rating_num" property="v:average">(?s:(.*?))</span>`)
	// 提取电影评分信息
	filmscore := res2.FindAllStringSubmatch(result, -1)
	// 解析， 编译正则表达式 -- 评分人数
	res3 := regexp.MustCompile(`<span>(.*?)人评价</span>`)
	// 提取评分人数信息
	people := res3.FindAllStringSubmatch(result, -1)

	//将筛选出的数据保存到文件中
	SaveTofile(filmname, filmscore, people, index)
	page <- index //执行到这里到时候，说明处理完毕了

}

//分析写入结果的函数
func SaveTofile(filmname, filmscore, people [][]string, index int) {
	//将爬取的数据保存在文件中
	f, err := os.Create("/Users/weiying/Desktop/crawler/" + "第" + strconv.Itoa(index) + "页" + ".txt")
	if err != nil {
		fmt.Println("os.Create error :", err)
		return
	}
	defer f.Close()
	// 先打印 抬头  电影名称        评分         评分人数
	f.WriteString("电影名称" + "\t\t\t" + "评分" + "\t\t" + "评分人数" + "\n")
	//len(filmname) 得到的是
	for i := 1; i <= len(filmname); i++ {
		f.WriteString("第" + strconv.Itoa(i) + "名：" + filmname[i][1] + "\t\t\t" +
			filmscore[i][1] + "\t\t\t" + people[i][1])
	}
}

func main() {
	var start, end int
	fmt.Printf("请输入起始页（>=1）：")
	fmt.Scan(&start)
	fmt.Printf("请输入结束页（>=start）：")
	fmt.Scan(&end)
	//封装函数，完成爬虫任务
	DB_worker(start, end)
}
```